# ðŸš€ LLM Proxy Gateway â€“ Multiâ€‘Provider LLM Orchestration Platform

## 1. Project Overview

This project implements a complete **LLM proxy gateway** designed to orchestrate multiple Large Language Model providers (OpenAI, Gemini, xAI, etc.) through a unified interface.

The objective is to:

- Provide a unified chat completion interface
- Provide a unified embeddings interface
- Simulate cost before execution (chat or embeddings)
- Evaluate model outputs with text metrics
- Offer both CLI and FastAPI usage
- Ensure reproducibility, logging, and clean architecture

The gateway abstracts provider differences and exposes a stable, extensible API layer.

---

## 2. Problem Statement

Modern LLM systems face several challenges:

- Multiple providers with different APIs
- Pricing differences per model and token type
- Token estimation inconsistencies
- Lack of cost visibility before execution
- No standardized evaluation layer
- Hardcoded provider logic in applications

This project addresses these constraints through:

- Provider dispatch abstraction
- JSON-based model & pricing catalogs
- Token estimation helpers (approximate + extensible)
- Pre-execution cost simulation
- Text metric evaluation layer
- Modular architecture (core / llm / utils)
- CLI + FastAPI interface

---

## 3. LLM Strategy

### Core Functional Dimensions

| Dimension | Description | Example |
|------------|------------|----------|
| provider | LLM provider backend | openai |
| model | Model name | gpt-4o-mini |
| input_tokens | Estimated prompt tokens | 1250 |
| output_tokens | Estimated completion tokens | 800 |
| total_cost_usd | Simulated or real cost | 0.0234 |
| evaluation_metric | Text similarity metric | f1_token |
| execution_mode | chat / embeddings | chat |

### Key Operational Objectives

| Objective | Why It Matters | Example Insight |
|------------|----------------|----------------|
| Cost transparency | Prevent unexpected billing | Compare providers before execution |
| Multi-provider fallback | Avoid vendor lock-in | Switch OpenAI â†’ Gemini |
| Embedding consistency | Unified vector generation | Same interface for all providers |
| Evaluation capability | Quantify model quality | F1 = 0.83 |
| Modular extensibility | Add providers safely | Future Claude / Mistral support |

---

## 4. Pipeline Architecture

```
	 User CLI / HTTP Request
            â†“
        Validation Layer (Pydantic)
            â†“
        Pipeline Orchestrator
            â†“
   Cost Simulation (optional)
            â†“
  Provider Dispatch (chat / embeddings)
            â†“
   Optional Evaluation Metrics
            â†“
      Structured JSON Response
```

---

## 5. Analytics & Evaluation Layer

The project provides built-in text evaluation metrics.

### Text Metrics Techniques

| Technique | Purpose | Example |
|------------|---------|----------|
| Exact Match | Strict equality | prediction == reference |
| Contains | Substring presence | "Paris" in response |
| F1 Token | Token-level precision/recall | F1 = 0.84 |
| Jaccard | Set similarity | 0.72 |
| Cosine Similarity | Vector similarity | 0.91 |
| ROUGE (optional) | Summarization quality | ROUGE-L |
| BLEU (optional) | N-gram overlap | BLEU-4 |
| BERTScore (optional) | Semantic similarity | 0.89 |

### Cost Simulation Logic

| Step | Purpose |
|------|----------|
| Approx token estimation | Fast, offline calculation |
| Folder scan (.txt) | Batch estimation |
| Chunking | Embedding window logic |
| Pricing lookup | JSON-based provider pricing |
| Cost math | $/1K tokens calculation |

---

## 6. Project Structure

```
llm-proxy-gateway/
â”œâ”€â”€ main.py                            ## CLI entry point (cost, run, evaluate, run-api) + uvicorn bootstrap
â”œâ”€â”€ menu_pipeline.sh                   ## Interactive CLI menu to run cost/pipeline/eval or API service
â”œâ”€â”€ requirements.txt                   ## Python dependencies
â”œâ”€â”€ README.md                          ## Project documentation
â”œâ”€â”€ .env                               ## Environment configuration (API keys, base urls, environment)
â”œâ”€â”€ .gitignore                         ## Git ignored files
â”œâ”€â”€ .dockerignore                      ## Docker build exclusions
â”‚
â”œâ”€â”€ docker/                            ## Container configuration and service orchestration
â”‚   â”œâ”€â”€ Dockerfile                     ## Application container definition
â”‚   â””â”€â”€ docker-compose.yml             ## Local orchestration (API + volumes + environment)
â”‚
â”œâ”€â”€ logs/                              ## Centralized runtime logs (application.log, etc.)
â”‚
â”œâ”€â”€ secrets/                           ## Service account credentials (excluded from version control)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           ## Raw .txt files for folder scan / evaluation corpora
â”‚   â””â”€â”€ processed/                     ## Optional CSV exports (per-file scan, results, etc.)
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”œâ”€â”€ models_catalog.json        ## Provider mapping + model names + defaults + context limits
â”‚   â”‚   â””â”€â”€ pricing_catalog.json       ## Pricing per provider/model ($/1K input, output, embeddings)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ swagger.yaml               ## OpenAPI spec (optional override / stable contract)
â”‚   â”‚
â”‚   â””â”€â”€ exports/                       ## Optional exports (CSV outputs, cost reports, evaluation reports)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_unit.py                   ## Unit tests for utils/costing/evaluation (no real HTTP calls)
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py                    ## Package marker
    â”œâ”€â”€ pipeline.py                    ## Orchestration: cost â†’ (embeddings | chat) â†’ optional eval/export
    â”‚
    â”œâ”€â”€ core/
    â”‚   â”œâ”€â”€ __init__.py                ## Core package marker
    â”‚   â”œâ”€â”€ service.py                 ## FastAPI app factory + routes (/healthcheck, /cost, /chat, /embeddings, /evaluation)
    â”‚   â”œâ”€â”€ schema.py                  ## Pydantic request/response models (API contract)
    â”‚   â”œâ”€â”€ config.py                  ## Settings loader (env parsing, paths, environment=dev/prod)
    â”‚   â””â”€â”€ errors.py                  ## Custom exceptions + helpers (log_and_raise_*)
    â”‚
    â”œâ”€â”€ llm/
    â”‚   â”œâ”€â”€ __init__.py                ## LLM package marker
    â”‚   â”œâ”€â”€ completion.py              ## Provider chat completion clients + dispatch (OpenAI/Gemini/xAI)
    â”‚   â”œâ”€â”€ embeddings.py              ## Provider embeddings clients + dispatch (OpenAI/Gemini/xAI)
    â”‚   â”œâ”€â”€ evaluation.py              ## Completion evaluation orchestration (calls metrics_utils)
    â”‚   â””â”€â”€ costing.py                 ## Cost simulation orchestration (catalog load, pricing resolution, calls scan helpers)
    â”‚
    â””â”€â”€ utils/
        â”œâ”€â”€ __init__.py                ## Utils package marker
        â”œâ”€â”€ logging_utils.py           ## Centralized logging + decorator (execution time + path on error)
        â”œâ”€â”€ utils.py                   ## Generic helpers: env, paths, safe IO, CLI input helpers, scan helpers, CSV export, basic stats
        â”œâ”€â”€ http_utils.py              ## Shared HTTP helpers (headers, payload builders, safe JSON logging)
        â”œâ”€â”€ metrics_utils.py           ## Text metrics (exact match, contains, F1 token, jaccard, cosine, ROUGE/BLEU/BERTScore optional)
        â”œâ”€â”€ tokeniser_utils.py         ## Tokenization + estimation helpers (approx + future provider tokenizers)
        â””â”€â”€ costing_utils.py           ## Pure costing helpers (pricing rows, chunking, token estimation for embeddings, cost math)
```

---

## 7. Prerequisites

- Python 3.10+
- Docker & Docker Compose
- API keys for desired LLM providers

### Ubuntu Example

```bash
sudo apt update
sudo apt install python3 python3-pip
python3 --version
```

---

## 8. Setup

### Python

```bash
python -m venv .llm_env
source .llm_env/bin/activate   							    ## for windows : .llm_env\Scripts\activate.bat
python -m pip install --upgrade pip setuptools wheel		## for windows : .llm_env\Scripts\python.exe -m pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
```

### Docker

```bash
docker compose build
docker compose up
```

---

## 9. Full System Verification

```bash
## Run API
python main.py --run-api

## Simulate chat cost
python main.py --simulate-cost --mode chat --providers openai --text "Hello"

## Simulate embeddings cost (folder)
python main.py --cost --mode embeddings --providers openai --path ./data/raw --recursive

## Run evaluation
python main.py --evaluate --predictions "Paris" --references "Paris"

## Run evaluation from file 
python main.py --evaluate --predictions-path ./data/raw/pred.txt --references-path ./data/raw/ref.txt

## Run test suite
pytest -q
```

---

## 10. Author

**Georges Nassopoulos**  
Email: georges.nassopoulos@gmail.com

